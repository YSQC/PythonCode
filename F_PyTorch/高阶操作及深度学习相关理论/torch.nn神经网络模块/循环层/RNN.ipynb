{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 100)\n",
    "\n",
    "rnn = nn.RNN(input_size=100, hidden_size=30)\n",
    "\n",
    "out, h = rnn(x) # 单层,序列长度为1\n",
    "print(out - h) # 此时out=h;即相当于RNNCell(只输出h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 10]) torch.Size([2, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=100, # 输入参数的词向量维度(也就是输入的特征通道)的大小\n",
    "             hidden_size=10, # 隐含变量的维度大小\n",
    "             num_layers=2, # 循环神经网络层的多少.若num_layers=2,则第2层的输入是第1层的输出(每一层都有一套参数,层与层之间的参数相互独立)\n",
    "             nonlinearity='tanh', # 非线性激活函数类型,也可以设置为'relu'.默认nonlinearity='tanh'\n",
    "             bias=True, # 是否添加偏置.默认bias=True\n",
    "             batch_first=False, # 如果batch_first=True,则输入张量大小为(N,T,C),而不是(T,N,C).默认batch_first=False\n",
    "             dropout=0.5) # 如果这个值非零,则在循环神经网络最后输出的基础上加上丢弃层,丢弃的概率由输入的dropout确定.默认dropout=0\n",
    "x = torch.randn(20, 3, 100) # 输入的默认形状为(T,N,C),其中T为序列的长度,N为min-batch的大小,C为输入的特征数目\n",
    "h_0 = torch.ones((2, 3, 10)) # (L*D, H, N),其中L为循环神经网络层数,D为1(单向)或2(双向)\n",
    "out, h = rnn(x, hx=h_0) # 自定h_0,默认h_0为全0张量\n",
    "print(out.shape, h.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0   shape= torch.Size([10, 100])\n",
      "weight_hh_l0   shape= torch.Size([10, 10])\n",
      "bias_ih_l0   shape= torch.Size([10])\n",
      "bias_hh_l0   shape= torch.Size([10])\n",
      "weight_ih_l1   shape= torch.Size([10, 10])\n",
      "weight_hh_l1   shape= torch.Size([10, 10])\n",
      "bias_ih_l1   shape= torch.Size([10])\n",
      "bias_hh_l1   shape= torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in rnn.named_parameters():\n",
    "    print(name, '  shape=', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 30]) torch.Size([4, 3, 15])\n"
     ]
    }
   ],
   "source": [
    "b_run = nn.RNN(input_size=100, hidden_size=15, num_layers=2,\n",
    "               bidirectional=True) # 是否为设置为双向循环神经网络,默认为False\n",
    "b_h_0 = torch.ones((4, 3, 15))\n",
    "b_out, b_h = b_run(x, hx=b_h_0)\n",
    "# b_out为每个序列最后一层的输出(双向则2维度*2);b_h为最后一个序列每层的输出(双向则0维度*2)\n",
    "print(b_out.shape, b_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0   shape= torch.Size([10, 100])\n",
      "weight_hh_l0   shape= torch.Size([10, 10])\n",
      "bias_ih_l0   shape= torch.Size([10])\n",
      "bias_hh_l0   shape= torch.Size([10])\n",
      "weight_ih_l0_reverse   shape= torch.Size([10, 100])\n",
      "weight_hh_l0_reverse   shape= torch.Size([10, 10])\n",
      "bias_ih_l0_reverse   shape= torch.Size([10])\n",
      "bias_hh_l0_reverse   shape= torch.Size([10])\n",
      "weight_ih_l1   shape= torch.Size([10, 20])\n",
      "weight_hh_l1   shape= torch.Size([10, 10])\n",
      "bias_ih_l1   shape= torch.Size([10])\n",
      "bias_hh_l1   shape= torch.Size([10])\n",
      "weight_ih_l1_reverse   shape= torch.Size([10, 20])\n",
      "weight_hh_l1_reverse   shape= torch.Size([10, 10])\n",
      "bias_ih_l1_reverse   shape= torch.Size([10])\n",
      "bias_hh_l1_reverse   shape= torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in b_run.named_parameters():\n",
    "    '''正向和反向两个方向的循环神经网络有各自的相互独立的参数'''\n",
    "    print(name, '  shape=', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.1584, -0.1181,  0.0134,  0.1998, -0.1996, -0.0111,  0.2989, -0.2947,\n          0.0927,  0.2011],\n        [ 0.2428,  0.1034,  0.2653, -0.2804, -0.3050, -0.0727, -0.1274, -0.2225,\n          0.1326,  0.2230],\n        [ 0.1868,  0.0359,  0.1400, -0.2635, -0.0658,  0.3000,  0.2791, -0.1634,\n         -0.2231,  0.0575],\n        [ 0.2148, -0.0505, -0.0739, -0.2656, -0.1210,  0.1496,  0.2525, -0.0506,\n          0.2353, -0.2564],\n        [-0.2597,  0.0110,  0.1036,  0.2018, -0.0318,  0.0163,  0.2232, -0.1560,\n         -0.2602, -0.2968],\n        [ 0.0404, -0.3090, -0.0423, -0.0009, -0.2997, -0.2353,  0.2185,  0.2978,\n         -0.0437, -0.1169],\n        [-0.1601,  0.2304, -0.0486, -0.2143, -0.2003,  0.0889,  0.1846,  0.1824,\n         -0.1103,  0.3064],\n        [ 0.0140, -0.0342,  0.2632,  0.0422, -0.2634,  0.1542,  0.0417, -0.1620,\n         -0.0876,  0.1856],\n        [ 0.1155, -0.2141,  0.1507,  0.2065,  0.0953, -0.2676, -0.0450, -0.1375,\n          0.2399, -0.1195],\n        [ 0.1261, -0.1421,  0.2973, -0.1373,  0.1706,  0.1216,  0.0445, -0.0399,\n         -0.1485, -0.1108]], requires_grad=True)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_run.weight_hh_l0 # all the weights and biases are initialized from U(-\\sqrt{k}, \\sqrt{k}), where k=1/hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}