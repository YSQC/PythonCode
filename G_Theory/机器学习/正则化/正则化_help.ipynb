{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&emsp;&emsp;正则化是结构风险最小化策略的实现,是在经验风险上加一个正则化项(regularizer)或罚项(penalty term).正则化\n",
    "项一般是模型复杂度的单调递增函数,模型越复杂,正则化值就越大.比如,正则化项可以是模型参数向量的范数.   \n",
    "&emsp;&emsp;正则化一般具有如下形式:    \n",
    "$$ \\min_{f \\in F}  \\frac{1}{N} \\sum_{i=1}^{N}L(y_i, f(\\mathbf{x}_i))  + \\lambda J(f) $$    \n",
    "其中,第一项是经验风险,第二项是正则化项目,$ \\lambda \\geq 0 $为调整两者之间关系的系数.    \n",
    "&emsp;&emsp;正则化项可以取不同的形式.例如,在回归问题中,损失函数是平方损失,正则化项可以是参数向量的$ L_2 $范数:   \n",
    "$$ L(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}{N} \\left(f(\\mathbf{x}_i;\\mathbf{w}) - y_i \\right)^2  + \\frac{\\lambda}{2} ||w||^2$$    \n",
    "这里,$ ||\\mathbf{w}|| $表示参数向量$\\mathbf{w}$的$L_2$范数.   \n",
    "&emsp;&emsp;正则化项可以是参数向量的$ L_1 $ 范数:     \n",
    "$$ L(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}{N} \\left(f(\\mathbf{x}_i;\\mathbf{w}) - y_i \\right)^2  + \\frac{\\lambda}{2} ||\\mathbf{w}||_1$$    \n",
    "这里,$ ||\\mathbf{w}|| $表示参数向量$\\mathbf{w}$的$ L_1$范数.    \n",
    "&emsp;&emsp;第一项的经验风险较小的模型可能较复杂(有多个非零参数),这时第二项的模型复杂度会较大.正则化的作用是选择经验风险与\n",
    "模型复杂度同时较小的模型.     \n",
    "&emsp;&emsp;正则化符号奥卡姆( Occam's razor)原理.奥卡姆剃刀原理应用于模型选择时变成以下想法:在所有可能选择的模型中,能够\n",
    "很好的解释已知数据并且十分简单才是最好的模型,也就是应该选择的模型.从贝叶斯估计的角度来看,正则化项对于与模型的先验概率.可以假定\n",
    "复杂的模型有较小的先验概率,简单的模型有较大的先验概率.       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}